<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">

<style type="text/css">
@font-face {
font-family: octicons-link;
src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}
body {
-webkit-text-size-adjust: 100%;
text-size-adjust: 100%;
color: #333;
font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
font-size: 16px;
line-height: 1.6;
word-wrap: break-word;
}
a {
background-color: transparent;
}
a:active,
a:hover {
outline: 0;
}
strong {
font-weight: bold;
}
h1 {
font-size: 2em;
margin: 0.67em 0;
}
img {
border: 0;
}
hr {
box-sizing: content-box;
height: 0;
}
pre {
overflow: auto;
}
code,
kbd,
pre {
font-family: monospace, monospace;
font-size: 1em;
}
input {
color: inherit;
font: inherit;
margin: 0;
}
html input[disabled] {
cursor: default;
}
input {
line-height: normal;
}
input[type="checkbox"] {
box-sizing: border-box;
padding: 0;
}
table {
border-collapse: collapse;
border-spacing: 0;
}
td,
th {
padding: 0;
}
* {
box-sizing: border-box;
}
input {
font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}
a {
color: #4078c0;
text-decoration: none;
}
a:hover,
a:active {
text-decoration: underline;
}
hr {
height: 0;
margin: 15px 0;
overflow: hidden;
background: transparent;
border: 0;
border-bottom: 1px solid #ddd;
}
hr:before {
display: table;
content: "";
}
hr:after {
display: table;
clear: both;
content: "";
}
h1,
h2,
h3,
h4,
h5,
h6 {
margin-top: 15px;
margin-bottom: 15px;
line-height: 1.1;
}
h1 {
font-size: 30px;
}
h2 {
font-size: 21px;
}
h3 {
font-size: 16px;
}
h4 {
font-size: 14px;
}
h5 {
font-size: 12px;
}
h6 {
font-size: 11px;
}
blockquote {
margin: 0;
}
ul,
ol {
padding: 0;
margin-top: 0;
margin-bottom: 0;
}
ol ol,
ul ol {
list-style-type: lower-roman;
}
ul ul ol,
ul ol ol,
ol ul ol,
ol ol ol {
list-style-type: lower-alpha;
}
dd {
margin-left: 0;
}
code {
font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
font-size: 12px;
}
pre {
margin-top: 0;
margin-bottom: 0;
font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}
.select::-ms-expand {
opacity: 0;
}
.octicon {
font: normal normal normal 16px/1 octicons-link;
display: inline-block;
text-decoration: none;
text-rendering: auto;
-webkit-font-smoothing: antialiased;
-moz-osx-font-smoothing: grayscale;
-webkit-user-select: none;
-moz-user-select: none;
-ms-user-select: none;
user-select: none;
}
.octicon-link:before {
content: '\f05c';
}
.markdown-body:before {
display: table;
content: "";
}
.markdown-body:after {
display: table;
clear: both;
content: "";
}
.markdown-body>*:first-child {
margin-top: 0 !important;
}
.markdown-body>*:last-child {
margin-bottom: 0 !important;
}
a:not([href]) {
color: inherit;
text-decoration: none;
}
.anchor {
display: inline-block;
padding-right: 2px;
margin-left: -18px;
}
.anchor:focus {
outline: none;
}
h1,
h2,
h3,
h4,
h5,
h6 {
margin-top: 1em;
margin-bottom: 16px;
font-weight: bold;
line-height: 1.4;
}
h1 .octicon-link,
h2 .octicon-link,
h3 .octicon-link,
h4 .octicon-link,
h5 .octicon-link,
h6 .octicon-link {
color: #000;
vertical-align: middle;
visibility: hidden;
}
h1:hover .anchor,
h2:hover .anchor,
h3:hover .anchor,
h4:hover .anchor,
h5:hover .anchor,
h6:hover .anchor {
text-decoration: none;
}
h1:hover .anchor .octicon-link,
h2:hover .anchor .octicon-link,
h3:hover .anchor .octicon-link,
h4:hover .anchor .octicon-link,
h5:hover .anchor .octicon-link,
h6:hover .anchor .octicon-link {
visibility: visible;
}
h1 {
padding-bottom: 0.3em;
font-size: 2.25em;
line-height: 1.2;
border-bottom: 1px solid #eee;
}
h1 .anchor {
line-height: 1;
}
h2 {
padding-bottom: 0.3em;
font-size: 1.75em;
line-height: 1.225;
border-bottom: 1px solid #eee;
}
h2 .anchor {
line-height: 1;
}
h3 {
font-size: 1.5em;
line-height: 1.43;
}
h3 .anchor {
line-height: 1.2;
}
h4 {
font-size: 1.25em;
}
h4 .anchor {
line-height: 1.2;
}
h5 {
font-size: 1em;
}
h5 .anchor {
line-height: 1.1;
}
h6 {
font-size: 1em;
color: #777;
}
h6 .anchor {
line-height: 1.1;
}
p,
blockquote,
ul,
ol,
dl,
table,
pre {
margin-top: 0;
margin-bottom: 16px;
}
hr {
height: 4px;
padding: 0;
margin: 16px 0;
background-color: #e7e7e7;
border: 0 none;
}
ul,
ol {
padding-left: 2em;
}
ul ul,
ul ol,
ol ol,
ol ul {
margin-top: 0;
margin-bottom: 0;
}
li>p {
margin-top: 16px;
}
dl {
padding: 0;
}
dl dt {
padding: 0;
margin-top: 16px;
font-size: 1em;
font-style: italic;
font-weight: bold;
}
dl dd {
padding: 0 16px;
margin-bottom: 16px;
}
blockquote {
padding: 0 15px;
color: #777;
border-left: 4px solid #ddd;
}
blockquote>:first-child {
margin-top: 0;
}
blockquote>:last-child {
margin-bottom: 0;
}
table {
display: block;
width: 100%;
overflow: auto;
word-break: normal;
word-break: keep-all;
}
table th {
font-weight: bold;
}
table th,
table td {
padding: 6px 13px;
border: 1px solid #ddd;
}
table tr {
background-color: #fff;
border-top: 1px solid #ccc;
}
table tr:nth-child(2n) {
background-color: #f8f8f8;
}
img {
max-width: 100%;
box-sizing: content-box;
background-color: #fff;
}
code {
padding: 0;
padding-top: 0.2em;
padding-bottom: 0.2em;
margin: 0;
font-size: 85%;
background-color: rgba(0,0,0,0.04);
border-radius: 3px;
}
code:before,
code:after {
letter-spacing: -0.2em;
content: "\00a0";
}
pre>code {
padding: 0;
margin: 0;
font-size: 100%;
word-break: normal;
white-space: pre;
background: transparent;
border: 0;
}
.highlight {
margin-bottom: 16px;
}
.highlight pre,
pre {
padding: 16px;
overflow: auto;
font-size: 85%;
line-height: 1.45;
background-color: #f7f7f7;
border-radius: 3px;
}
.highlight pre {
margin-bottom: 0;
word-break: normal;
}
pre {
word-wrap: normal;
}
pre code {
display: inline;
max-width: initial;
padding: 0;
margin: 0;
overflow: initial;
line-height: inherit;
word-wrap: normal;
background-color: transparent;
border: 0;
}
pre code:before,
pre code:after {
content: normal;
}
kbd {
display: inline-block;
padding: 3px 5px;
font-size: 11px;
line-height: 10px;
color: #555;
vertical-align: middle;
background-color: #fcfcfc;
border: solid 1px #ccc;
border-bottom-color: #bbb;
border-radius: 3px;
box-shadow: inset 0 -1px 0 #bbb;
}
.pl-c {
color: #969896;
}
.pl-c1,
.pl-s .pl-v {
color: #0086b3;
}
.pl-e,
.pl-en {
color: #795da3;
}
.pl-s .pl-s1,
.pl-smi {
color: #333;
}
.pl-ent {
color: #63a35c;
}
.pl-k {
color: #a71d5d;
}
.pl-pds,
.pl-s,
.pl-s .pl-pse .pl-s1,
.pl-sr,
.pl-sr .pl-cce,
.pl-sr .pl-sra,
.pl-sr .pl-sre {
color: #183691;
}
.pl-v {
color: #ed6a43;
}
.pl-id {
color: #b52a1d;
}
.pl-ii {
background-color: #b52a1d;
color: #f8f8f8;
}
.pl-sr .pl-cce {
color: #63a35c;
font-weight: bold;
}
.pl-ml {
color: #693a17;
}
.pl-mh,
.pl-mh .pl-en,
.pl-ms {
color: #1d3e81;
font-weight: bold;
}
.pl-mq {
color: #008080;
}
.pl-mi {
color: #333;
font-style: italic;
}
.pl-mb {
color: #333;
font-weight: bold;
}
.pl-md {
background-color: #ffecec;
color: #bd2c00;
}
.pl-mi1 {
background-color: #eaffea;
color: #55a532;
}
.pl-mdr {
color: #795da3;
font-weight: bold;
}
.pl-mo {
color: #1d3e81;
}
kbd {
display: inline-block;
padding: 3px 5px;
font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
line-height: 10px;
color: #555;
vertical-align: middle;
background-color: #fcfcfc;
border: solid 1px #ccc;
border-bottom-color: #bbb;
border-radius: 3px;
box-shadow: inset 0 -1px 0 #bbb;
}
.task-list-item {
list-style-type: none;
}
.task-list-item+.task-list-item {
margin-top: 3px;
}
.task-list-item input {
margin: 0 0.35em 0.25em -1.6em;
vertical-align: middle;
}
:checked+.radio-label {
z-index: 1;
position: relative;
border-color: #4078c0;
}
.sourceLine {
display: inline-block;
}
code .kw { color: #000000; }
code .dt { color: #ed6a43; }
code .dv { color: #009999; }
code .bn { color: #009999; }
code .fl { color: #009999; }
code .ch { color: #009999; }
code .st { color: #183691; }
code .co { color: #969896; }
code .ot { color: #0086b3; }
code .al { color: #a61717; }
code .fu { color: #63a35c; }
code .er { color: #a61717; background-color: #e3d2d2; }
code .wa { color: #000000; }
code .cn { color: #008080; }
code .sc { color: #008080; }
code .vs { color: #183691; }
code .ss { color: #183691; }
code .im { color: #000000; }
code .va {color: #008080; }
code .cf { color: #000000; }
code .op { color: #000000; }
code .bu { color: #000000; }
code .ex { color: #000000; }
code .pp { color: #999999; }
code .at { color: #008080; }
code .do { color: #969896; }
code .an { color: #008080; }
code .cv { color: #008080; }
code .in { color: #008080; }
</style>
<style>
body {
box-sizing: border-box;
min-width: 200px;
max-width: 980px;
margin: 0 auto;
padding: 45px;
padding-top: 0px;
}
</style>


</head>

<body>

<h1 id="non-centered-parameterisation-in-hierarchical-bayesian-models-not-just-for-univariate-gaussians">Non-Centered
Parameterisation in Hierarchical Bayesian Models: Not Just For
Univariate Gaussians</h1>
<p>2024-10-01</p>
<h2 id="what-is-a-hierarchical-bayesian-model">What is a Hierarchical
Bayesian Model?</h2>
<p>Hierarchical Bayesian models assess individual variation (e.g.,
between people) in a parameter or quantity of interest while
simultaneously producing insights about group or population level
effects. This type of model is useful because it avoids the need to a)
assume the parameter or quantity of interest is identical across the
population or b) assume individuals within the same population are
completely independent. The hierarchical model recognises that
individuals within a population are likely to share certain common
characteristics with other members of that population, but also
recognises the presence of individual variation within that
population.</p>
<p>Suppose we want to examine the impact of weekly working hours on
mental health in a particular population, and that we have multilevel
data where multiple observations of both weekly working hours and mental
health were taken over time for a set of individuals. A hierarchical
Bayesian model would allow us to estimate the impact of working hours on
mental health for each individual. However, each individual’s estimated
effect would be informed not only by that individual’s data, but also in
part by the estimated effects from other individual’s in the population.
This phenomenon is sometimes referred to as ‘partial pooling’ of
information. It means that information about the parameter or quantity
of interest (in this case, the effect of working hours on mental health)
is shared or ‘pooled’ across individuals in the sample. The ‘partial’
bit means that an individual’s estimate is not completely determined by
the information about other individuals’ estimates. The model leaves
room for individual variation by estimating unique effects for each
individual.</p>
<p>The usefulness of hierarchical Bayesian models is well documented
elsewhere (e.g., see <a href="https://pubmed.ncbi.nlm.nih.gov/29595295/">here</a>, <a href="https://link.springer.com/article/10.3758/s13428-018-1054-3">here</a>,
or for work from my own lab see <a href="https://link.springer.com/article/10.1007/s42113-023-00173-6">here</a>
). The point of this post is not to further highlight the virtues of
these models. The point is to explain one particularly tricky aspects of
actually implementing them.</p>
<h2 id="what-is-a-non-centered-parameterisation">What is a Non-Centered
Parameterisation?</h2>
<p>To explain what a non-centered parameterisation is and why it’s
useful, let’s start with a simple example. Let’s simulate some
multilevel data. In this example, we’ll assume we have 500 participants
who each produce 5 observations of three variables: X1, X2, and Y. In
the code below, the parameters beta0, beta1, beta2, and sigma represent
the intercept, linear effect of X1, linear effect of X2, and the
residual error respectively. We first assign population-level
distributions representing the variation in these parameters across
participants. We then randomly sample the parameter values for each
participant based on the population-level distributions we defined.
Finally, we use the participant-level parameters and the covariates X1
and X2 to randomly generate response variable Y, before creating a list
containing that data that can be read into stan.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">2500</span>  <span class="co"># Total number of observations</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>J <span class="ot">&lt;-</span> <span class="dv">500</span>    <span class="co"># Number of participants</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>n_j <span class="ot">&lt;-</span> N <span class="sc">/</span> J  <span class="co"># Number of observations per participant</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># Generate participant IDs and predictor variables</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>participant_id <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>J, <span class="at">each =</span> n_j)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co"># Population parameter values</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>beta0_mean <span class="ot">&lt;-</span> <span class="fl">2.5</span>   <span class="co"># beta0 population mean</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>beta1_mean <span class="ot">&lt;-</span> <span class="fl">1.5</span>   <span class="co"># Effect of X1 population mean</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>beta2_mean <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.2</span>  <span class="co"># Effect of X2 population mean</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>beta0_sd <span class="ot">&lt;-</span> <span class="fl">1.5</span>     <span class="co"># beta0 population SD</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>beta1_sd <span class="ot">&lt;-</span> <span class="fl">1.3</span>     <span class="co"># Effect of X1  population SD</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>beta2_sd <span class="ot">&lt;-</span> <span class="fl">0.7</span>     <span class="co"># Effect of X2 population SD</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>sigma_mean <span class="ot">&lt;-</span> <span class="fl">0.7</span>   <span class="co"># Residual mean</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>sigma_sd <span class="ot">&lt;-</span> <span class="fl">0.3</span>     <span class="co"># Residual SD</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="co"># Simulate variability in parameters across participants</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> J, <span class="at">mean =</span> beta0_mean, <span class="at">sd =</span> beta0_sd)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> J, <span class="at">mean =</span> beta1_mean, <span class="at">sd =</span> beta1_sd)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> J, <span class="at">mean =</span> beta2_mean, <span class="at">sd =</span> beta2_sd)</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> truncnorm<span class="sc">::</span><span class="fu">rtruncnorm</span>(<span class="at">n =</span> J, <span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="cn">Inf</span>, <span class="at">mean =</span> sigma_mean, <span class="at">sd =</span> sigma_sd)</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="co"># Generate response variable</span></span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rep</span>(beta0, <span class="at">each =</span> n_j) <span class="sc">+</span> </span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>  <span class="fu">rep</span>(beta1, <span class="at">each =</span> n_j) <span class="sc">*</span> X1 <span class="sc">+</span> </span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>  <span class="fu">rep</span>(beta2, <span class="at">each =</span> n_j) <span class="sc">*</span> X2 <span class="sc">+</span> </span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>  <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">rep</span>(sigma, <span class="at">each =</span> n_j))</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a><span class="co"># Prepare data for Stan</span></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>stan_data <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>  <span class="at">N =</span> N,</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>  <span class="at">J =</span> J,</span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>  <span class="at">X1 =</span> X1,</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>  <span class="at">X2 =</span> X2,</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>  <span class="at">participant_id =</span> participant_id</span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>)</span></code></pre></div>
<p>Now let’s define a hierarchical Bayesian model of this data in stan.
What makes the model below hierarchical is that the</p>
<!-- priors on the participant-level parameters defined in the model section depend on the population-level parameters, which themselves are uncertain and therefore estimated by the model. Information from each participant's data flows "up the hierarchy" influencing the population-level parameters via the individual-specific parameters. But information about the population parameters in turn flows back down the hierarchy by constraining the individual parameters to values that are more plausible given the distribution of those parameters in the population. -->

<!-- ```{r model-1,messages=FALSE,warnings=FALSE,results='hide'} -->

<!-- stan_model <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   real beta0_mean;            // Intercept population mean -->

<!--   real beta1_mean;            // Effect of X1 population mean -->

<!--   real beta2_mean;            // Effect of X2 population mean -->

<!--   real<lower=0> beta0_sd;     // Intercept population SD -->

<!--   real<lower=0> beta1_sd;     // Effect of X1 population SD -->

<!--   real<lower=0> beta2_sd;     // Effect of X2 population SD -->

<!--   real<lower=0> sigma_mean;   // Residual mean -->

<!--   real<lower=0> sigma_sd;     // Residual SD -->

<!--   vector[J] beta0;            // Participant intercepts -->

<!--   vector[J] beta1;            // Participant X1 effects -->

<!--   vector[J] beta2;            // Participant X2 effects -->

<!--   vector<lower=0>[J] sigma;   // Participant residuals -->

<!-- } -->

<!-- model { -->

<!--   // Population-level Priors -->

<!--   beta0_mean ~ normal(0, 5); -->

<!--   beta1_mean ~ normal(0, 5); -->

<!--   beta2_mean ~ normal(0, 5); -->

<!--   beta0_sd ~ normal(0, 2.5); -->

<!--   beta1_sd ~ normal(0, 2.5); -->

<!--   beta2_sd ~ normal(0, 2.5); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level Priors -->

<!--   beta0 ~ normal(beta0_mean,beta0_sd); -->

<!--   beta1 ~ normal(beta1_mean,beta1_sd); -->

<!--   beta2 ~ normal(beta2_mean,beta2_sd); -->

<!--   sigma ~ normal(sigma_mean,sigma_sd); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- } -->

<!-- " -->

<!-- ``` -->

<!-- We can fit the model using the code below. -->

<!-- ```{r fit-model-1} -->

<!-- # Compile the model -->

<!-- mod <- cmdstan_model(write_stan_file(stan_model)) -->

<!-- # Fit the model -->

<!-- fit <- mod$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- As you can see from the message after the model finished, there were a lot of divergent transitions. This means the sampler is having difficulty exploring the posterior distribution effectively. This is common when hierarchical models are specified this way, because the scale of each participant-level parameter depends on another parameter in the model. This dependency can create a posterior geometry that is tricky to sample from efficiently. -->

<!-- This is where the **non-centered parameterisation** can be helpful^[I realise there's a departure from Australian english in writing 'centered' instead of 'centred', but the latter just looks strange to me. So I'll use 'centered'.]. It removes this dependency by reparameterising the participant-level parameters. In the model below, a non-centered parameterisation is applied to beta0, beta1, and beta2 (sigma is a little more complicated since it's bounded at 0. We'll get to that next). As you can see, we now estimate z-scores for these three parameters and then in the `transformed parameters` block unstandardise the parameter via an inverse-z transform. -->

<!-- ```{r model-2,messages=FALSE,warnings=FALSE} -->

<!-- stan_model_nc <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   real beta0_mean;            // Intercept population mean -->

<!--   real beta1_mean;            // Effect of X1 population mean -->

<!--   real beta2_mean;            // Effect of X2 population mean -->

<!--   real<lower=0> beta0_sd;     // Intercept population SD -->

<!--   real<lower=0> beta1_sd;     // Effect of X1 population SD -->

<!--   real<lower=0> beta2_sd;     // Effect of X2 population SD -->

<!--   real<lower=0> sigma_mean;   // Residual mean -->

<!--   real<lower=0> sigma_sd;     // Residual SD -->

<!--   vector[J] beta0_z;          // Participant intercepts (z-score) -->

<!--   vector[J] beta1_z;          // Participant X1 effects (z-score) -->

<!--   vector[J] beta2_z;          // Participant X2 effects (z-score) -->

<!--   vector<lower=0>[J] sigma;   // Participant residuals -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[J] beta0 = beta0_mean + beta0_sd * beta0_z; -->

<!--   vector[J] beta1 = beta1_mean + beta1_sd * beta1_z; -->

<!--   vector[J] beta2 = beta2_mean + beta2_sd * beta2_z; -->

<!-- } -->

<!-- model { -->

<!--   // Population-level Priors -->

<!--   beta0_mean ~ normal(0, 5); -->

<!--   beta1_mean ~ normal(0, 5); -->

<!--   beta2_mean ~ normal(0, 5); -->

<!--   beta0_sd ~ normal(0, 2.5); -->

<!--   beta1_sd ~ normal(0, 2.5); -->

<!--   beta2_sd ~ normal(0, 2.5); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level Priors -->

<!--   beta0 ~ std_normal(); -->

<!--   beta1 ~ std_normal(); -->

<!--   beta2 ~ std_normal(); -->

<!--   sigma ~ normal(sigma_mean,sigma_sd); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- } -->

<!-- " -->

<!-- # Compile the model -->

<!-- mod_nc <- cmdstan_model(write_stan_file(stan_model_nc)) -->

<!-- # Fit the model -->

<!-- fit_nc <- mod_nc$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- We're still getting a lot of divergences and we're hitting the maximum treedepth on many iterations, which suggests that the model still isn't sampling efficiently. This is because we still haven't applied an uncentered parameterisation to `sigma`. We'll do that next. -->

<!-- ## Non-Centered Parameterisation for Truncated Parameters -->

<!-- What makes `beta0`, `beta1`, and `beta2` easy to reparameterise is the fact that these variables can take on any real value. In other words, they're not bounded or constrained to a particular range. So we don't have to worry about the result of the inverse-z transform satisfying a particular constraint. But not all parameters are unbounded. A good example of a bounded parameter is a standard deviation. Standard deviations must be positive. So when estimating these parameters, a common approach is to sample from *truncated distributions* that are constrained to have a lower bound of 0 (as we have in the models above). It's not immediately obvious how the non-centered parameterisation can be applied to parameters that are bounded. -->

<!-- As it turns out, it's actually fairly straightfoward. The trick is to sample the parameter as if it were unconstrained and then convert it via transformation to impose the appropriate constraints. Let's assume we want to apply a non-centered transformation to `sigma` that accounts for the constraint that this parameter should be positive. We can do something like what's done in the R code below. Here, we use the `exp()` function to exponentiate the result of the inverse-z transform, which maps `sigma` to the positive real numbers. Technically, this transformation means that `sigma` is lognormally distributed (in other words, the log of `sigma` is normally distributed). -->

<!-- ```{r sigma-demo-1} -->

<!-- n = 10000                                      #number of samples -->

<!-- sigma_mean = rnorm(n)                          #sample sigma mean -->

<!-- sigma_sd = rtruncnorm(n,a=0)                   #sample sigma sd -->

<!-- sigma_z = rnorm(n)                             #sample sigma z-score -->

<!-- sigma = exp(sigma_mean + sigma_sd * sigma_z)   #unstandardise and convert to positive via exponentiation -->

<!-- hist(sigma) -->

<!-- density(sigma) -->

<!-- ``` -->

<!-- Notice in the output above, however, that the resulting distribution of `sigma` is heavily skewed. This happens because of the exponential transformation. Values that are on the high end of the distribution before the exponentiation get pulled way out when the transformation is applied. It only takes an untransformed value of 10 to produce a transformed value of more than 20,000. A prior that is this heavily skewed can be difficult to sample from. So this transformation may not help us much. This skew can be alleviated to some extent by placing different priors on `sigma_mean` and `sigma_sd`, but a big part of the problem is the exponentiation itself. -->

<!-- Importantly, there are other transformations that we can apply. One that I particularly like is the *softplus* transformation $f(x) = \log(1+e^x)$. This transformation avoids the heavy skew that can sometimes be created by simply exponentiating. Compare the distribution above with the one below. -->

<!-- ```{r sigma-demo-2} -->

<!-- n = 10000                                             #number of samples -->

<!-- sigma_mean = rnorm(n)                                 #sample sigma mean -->

<!-- sigma_sd = rtruncnorm(n,a=0)                          #sample sigma sd -->

<!-- sigma_z = rnorm(n)                                    #sample sigma z-score -->

<!-- sigma = log(1+exp(sigma_mean + sigma_sd * sigma_z))   #unstandardise and convert to positive via softplus -->

<!-- hist(sigma) -->

<!-- density(sigma) -->

<!-- ``` -->

<!-- This second distribution is much less skewed and will be easier to sample from. As with the non-centered parameterisation applied to uncontained parameters, you can change the priors on the distribution by modifying the priors on the population parameters. If you want truncate the distribution at a value other than zero, all you need to do is add a constant. The example below truncates the distribution at 5. -->

<!-- ```{r sigma-demo-3} -->

<!-- n = 10000                                               #number of samples -->

<!-- sigma_mean = rnorm(n)                                   #sample sigma mean -->

<!-- sigma_sd = rtruncnorm(n,a=0)                            #sample sigma sd -->

<!-- sigma_z = rnorm(n)                                      #sample sigma z-score -->

<!-- sigma = 5+log(1+exp(sigma_mean + sigma_sd * sigma_z))   #unstandardise and convert using softplus -->

<!-- hist(sigma) -->

<!-- density(sigma) -->

<!-- ``` -->

<!-- Alternatively, to make the truncation point an *upper bound* instead of a lower bound, simply multiply the result of the transformation by -1 as in the example below. -->

<!-- ```{r sigma-demo-4} -->

<!-- n = 10000                                               #number of samples -->

<!-- sigma_mean = rnorm(n)                                   #sample sigma mean -->

<!-- sigma_sd = rtruncnorm(n,a=0)                            #sample sigma sd -->

<!-- sigma_z = rnorm(n)                                      #sample sigma z-score -->

<!-- sigma = 5-log(1+exp(sigma_mean + sigma_sd * sigma_z))   #unstandardise and convert using softplus -->

<!-- hist(sigma) -->

<!-- density(sigma) -->

<!-- ``` -->

<!-- Here's a model that uses the softplus transformation to apply a non-centered parameterisation to the `sigma` parameter. As you can see, the process is identical to how we reparameterise the `beta` parameters except that the softplus transformation is applied to the parameter after the inverse-z transform is applied. -->

<!-- ```{r model-3,messages=FALSE,warnings=FALSE} -->

<!-- stan_model_ncs <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   real beta0_mean;            // Intercept population mean -->

<!--   real beta1_mean;            // Effect of X1 population mean -->

<!--   real beta2_mean;            // Effect of X2 population mean -->

<!--   real<lower=0> beta0_sd;     // Intercept population SD -->

<!--   real<lower=0> beta1_sd;     // Effect of X1 population SD -->

<!--   real<lower=0> beta2_sd;     // Effect of X2 population SD -->

<!--   real sigma_mean;            // Residual population mean (before transformation) -->

<!--   real<lower=0> sigma_sd;     // Residual population SD (before transformation) -->

<!--   vector[J] beta0_z;          // Participant intercepts (z-score) -->

<!--   vector[J] beta1_z;          // Participant X1 effects (z-score) -->

<!--   vector[J] beta2_z;          // Participant X2 effects (z-score) -->

<!--   vector[J] sigma_z;          // Participant residuals (z-score, before transformation) -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[J] beta0 = beta0_mean + beta0_sd * beta0_z; -->

<!--   vector[J] beta1 = beta1_mean + beta1_sd * beta1_z; -->

<!--   vector[J] beta2 = beta2_mean + beta2_sd * beta2_z; -->

<!--   vector[J] sigma = log1p_exp(sigma_mean + sigma_sd * sigma_z); -->

<!-- } -->

<!-- model { -->

<!--   // Population-level Priors -->

<!--   beta0_mean ~ normal(0, 5); -->

<!--   beta1_mean ~ normal(0, 5); -->

<!--   beta2_mean ~ normal(0, 5); -->

<!--   beta0_sd ~ normal(0, 2.5); -->

<!--   beta1_sd ~ normal(0, 2.5); -->

<!--   beta2_sd ~ normal(0, 2.5); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level Priors -->

<!--   beta0_z ~ std_normal(); -->

<!--   beta1_z ~ std_normal(); -->

<!--   beta2_z ~ std_normal(); -->

<!--   sigma_z ~ std_normal(); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- } -->

<!-- " -->

<!-- # Compile the model -->

<!-- mod_ncs <- cmdstan_model(write_stan_file(stan_model_ncs)) -->

<!-- # Fit the model -->

<!-- fit_ncs <- mod_ncs$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- You can see from the output that there is only a small number divergent transitions, which are rare enough that they shouldn't pose any challenges for interpreting the results (these can probably be further reduced by increasing `adapt_delta` above it's default value of 0.8). There are also no more instances of the maximum treedepth being reached. The summary statistics and traceplot of the population parameters show the model has converged. -->

<!-- ```{r model-3-summary,messages=FALSE,warnings=FALSE} -->

<!-- parameters = c("beta0_mean", "beta1_mean","beta2_mean", "beta0_sd", "beta1_sd","beta2_sd","sigma_mean","sigma_sd","lp__") -->

<!-- fit_ncs$summary(variables = parameters) -->

<!-- mcmc_trace(fit_ncs$draws(variables = parameters)) -->

<!-- ``` -->

<!-- It's also possible to apply non-centered transformations to double bounded parameters that have lower bounds *and* upper bounds (e.g., probability parameters that are constrained between 0 and 1). But there's a bit more to think about with double bounded parameters, so I think I'll address those in a future post. What I really want to get to in this post is how to apply non-centered transformations to multivariate distributions. -->

<!-- ## Non-Centered Parameterisation for Multivariate Distributions -->

<!-- The models presented above assume that individual variation in the parameters are uncorrelated. In other words, knowing one person's value of `beta0` gives us no information about their plausible `beta1` and `beta2` values. However, in many cases, it's reasonable to assume these quantities are correlated. Returning to the example question of how the number of hours we spend working affects our mental health, it's plausible that those with lower levels of mental health overall suffer more from working longer hours. To examine this possibility, we need to allow for the individual-specific parameters to be correlated. First, let's simulate some data where this is the case. The code below simulates data from a model where `beta0`, `beta1`, and `beta2` are correlated, with `Rho` containing the parameter correlation matrix. In principle, we could also allow the untransformed version of `sigma` to also correlate with the `beta` parameters. But the transformation applied to sigma makes the interpretation of this correlation less straightforward. So we'll keep it simple for now and assume `sigma` does not correlate with the `beta` parameters. -->

<!-- ```{r simulate-data-2,messages=FALSE,warnings=FALSE} -->

<!-- # Set random seed for reproducibility -->

<!-- set.seed(123) -->

<!-- # Simulate data -->

<!-- N <- 2500  # Total number of observations -->

<!-- J <- 500    # Number of participants -->

<!-- n_j <- N / J  # Number of observations per participant -->

<!-- # Generate participant IDs and predictor variables -->

<!-- participant_id <- rep(1:J, each = n_j) -->

<!-- X1 <- rnorm(N, mean = 0, sd = 1) -->

<!-- X2 <- rnorm(N, mean = 0, sd = 1) -->

<!-- # Population parameter values -->

<!-- beta0_mean <- 2.5   # beta0 population mean -->

<!-- beta1_mean <- 1.5   # Effect of X1 population mean -->

<!-- beta2_mean <- -0.2  # Effect of X2 population mean -->

<!-- beta0_sd <- 1.5     # beta0 population SD -->

<!-- beta1_sd <- 1.3     # Effect of X1  population SD -->

<!-- beta2_sd <- 0.7     # Effect of X2 population SD -->

<!-- sigma_mean <- 0.7   # Residual mean -->

<!-- sigma_sd <- 0.3     # Residual SD -->

<!-- #Correlation matrix of individual parameters -->

<!-- Rho <- matrix(c(1,0.5,-0.5, -->

<!--                 0.5,1,0.4, -->

<!--                -0.5,0.4,1),nrow=3,byrow=3) -->

<!-- #Diagonal matrix of population standard deviations -->

<!-- D <- diag(c(beta0_sd,beta1_sd,beta2_sd)) -->

<!-- #Population covariance matrix -->

<!-- Sigma = D %*% Rho %*% D -->

<!-- #Vector of population means -->

<!-- Mu = c(beta0_mean,beta1_mean,beta2_mean) -->

<!-- # Generate correlated participant-specific parameters -->

<!-- theta <- MASS::mvrnorm(J, Mu, Sigma) -->

<!-- # Check correlation -->

<!-- cor(theta) -->

<!-- # Calculate participant-specific beta0s and slopes -->

<!-- beta0 <- theta[,1] -->

<!-- beta1 <- theta[,2] -->

<!-- beta2 <- theta[,3] -->

<!-- # Simulate variability in sigma -->

<!-- sigma <- rtruncnorm(n = N, a = 0, b = Inf, mean = sigma_mean, sd = sigma_sd) -->

<!-- # Generate response variable -->

<!-- Y <- rep(beta0, each = n_j) + -->

<!--   rep(beta1, each = n_j) * X1 + -->

<!--   rep(beta2, each = n_j) * X2 + -->

<!--   rnorm(N, mean = 0, sd = rep(sigma, each = n_j)) -->

<!-- # Prepare data for Stan -->

<!-- stan_data <- list( -->

<!--   N = N, -->

<!--   J = J, -->

<!--   X1 = X1, -->

<!--   X2 = X2, -->

<!--   Y = Y, -->

<!--   participant_id = participant_id -->

<!-- ) -->

<!-- ``` -->

<!-- Let's now tweak the model above to account for the covariation between these parameters. As with the uncorrelated versions above, there are a number of ways we can parameterise the model. In the version below, we directly estimate `population_cov`, which is the population covariance matrix of the individual-level parameters. The presence of this covariance matrix increases the amount of information pooling the happens when estimating the model. Whereas in the above models, a participant's estimate of a given parameter was constrained by other participants' estimates *of that same parameter*, now the estimate of each `beta` parameter is influenced by other participants' estimates of not only that same parameter but also of estimates of the other `beta` parameters. This extra pooling of information is especially helpful when there are relatively few observations per individual. The prior distribution of this covariance matrix is an inverse-wishart distribution, a commonly used prior for estimating covariance matrices, with 4 degrees of freedom and an identity scale matrix. The model also includes a `generated quantities` block that converts the population covariance matrix into a vector of population SDs and a population correlation matrix. -->

<!-- ```{r fit-model-4,messages=FALSE,warnings=FALSE} -->

<!-- stan_model_mv_cov <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   vector[3] population_means;          // Population means for beta0, beta1, beta2 -->

<!--   cov_matrix[3] population_cov;        // Covariance matrix for beta0, beta1, and beta2 -->

<!--   array[J] vector[3] theta;            // Individual-level beta0, beta1, and beta2 estimates -->

<!--   real sigma_mean;                     // Residual population mean (before transformation) -->

<!--   real<lower=0> sigma_sd;              // Residual population SD (before transformation) -->

<!--   vector[J] sigma_z;                   // Individual-level residuals (z-score, before transformation) -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[J] sigma = log1p_exp(sigma_mean + sigma_sd * sigma_z); -->

<!--   vector[J] beta0 = to_vector(theta[,1]); -->

<!--   vector[J] beta1 = to_vector(theta[,2]); -->

<!--   vector[J] beta2 = to_vector(theta[,3]); -->

<!-- } -->

<!-- model { -->

<!--   // Population-level priors -->

<!--   population_means ~ normal(0, 5); -->

<!--   population_cov ~ inv_wishart(4,identity_matrix(3)); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level priors -->

<!--   theta ~ multi_normal(population_means, population_cov); -->

<!--   sigma_z ~ std_normal(); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- } -->

<!-- generated quantities { -->

<!--   //Convert population covariance matrix to population correlation matrix -->

<!--   vector[3] population_sds = sqrt(diagonal(population_cov)); //extract variances and convert to SDs -->

<!--   //The code below equates to: diag_matrix(population_sds)^-1 *  population_cov * diag_matrix(population_sds)^-1 -->

<!--   corr_matrix[3] population_corr = mdivide_right_spd(mdivide_left_spd(diag_matrix(population_sds),population_cov),diag_matrix(population_sds)); -->

<!-- }" -->

<!-- # Compile the model -->

<!-- mod_mv_cov <- cmdstan_model(write_stan_file(stan_model_mv_cov)) -->

<!-- # Fit the model -->

<!-- fit_mv_cov <- mod_mv_cov$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- The model below shows another way to parameterise a model with correlated individual-level parameters. This version decouples the standard deviations of the population distributions from the correlations. Separating the standard deviations from the correlations in this way avoids the problem of the scale of the population distributions influencing the degree of covariation between the parameters, which can happen when the covariance matrix is directly estimated. The major change in this model is that we separately estimate `population_corr` which is the population correlation matrix of the individual-level parameters and `population_sds` which is the standard deviation of the population distributions. The code `quad_form_diag(population_corr, population_sds)` converts the correlation matrix and vector of SDs to a covariance matrix, which is needed to compute the PDF of the multivariate normal distribution (this is the opposite operation to what is done in the `generated quantities` block of the model above). The prior distribution of this correlation matrix is an LKJ distribution with a concentration parameter of 1, which is a uniform prior across all possible correlation matrices. -->

<!-- ```{r fit-model-5,messages=FALSE,warnings=FALSE} -->

<!-- stan_model_mv_cor <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   vector[3] population_means;          // Population means for beta0, beta1, beta2 -->

<!--   vector<lower=0>[3] population_sds;   // Population SDs for beta0, beta1, beta2 -->

<!--   corr_matrix[3] population_corr;      // Correlation matrix for beta0, beta1, and beta2 -->

<!--   array[J] vector[3] theta;            // Individual-level beta0, beta1, and beta2 estimates -->

<!--   real sigma_mean;                     // Residual population mean (before transformation) -->

<!--   real<lower=0> sigma_sd;              // Residual population SD (before transformation) -->

<!--   vector[J] sigma_z;                   // Individual-level residuals (z-score, before transformation) -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[J] sigma = log1p_exp(sigma_mean + sigma_sd * sigma_z); -->

<!--   vector[J] beta0 = to_vector(theta[,1]); -->

<!--   vector[J] beta1 = to_vector(theta[,2]); -->

<!--   vector[J] beta2 = to_vector(theta[,3]); -->

<!-- } -->

<!-- model { -->

<!--   // Population-level priors -->

<!--   population_means ~ normal(0, 5); -->

<!--   population_sds ~ normal(0, 2.5); -->

<!--   population_corr ~ lkj_corr(1); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level priors -->

<!--   theta ~ multi_normal(population_means, quad_form_diag(population_corr, population_sds)); -->

<!--   sigma_z ~ std_normal(); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- }" -->

<!-- # Compile the model -->

<!-- mod_mv_cor <- cmdstan_model(write_stan_file(stan_model_mv_cor)) -->

<!-- # Fit the model -->

<!-- fit_mv_cor <- mod_mv_cor$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- Finally, the version below extends on the version above and implements a non-centered version of this model. The first change here is that we're no longer estimating the correlation matrix. Instead, we're estimating the cholesky factor of the correlation matrix, which makes things computationally simpler (we adopt the commonly used `L_` notation to denote cholesky factors here). Once we compute the cholesky factor of the covariance matrix, we apply the multivariate version of the inverse-z transform to compute the values of `beta0`, `beta1`, and `beta2`. And then we can compute the likelihood in the same way we have in the previous model. The model below also includes a `generated quantities` block which converts the cholesky factor of the population correlation matrix to the raw correlation matrix, so that these correlations can be more easily interpreted. -->

<!-- ```{r fit-model-6,messages=FALSE,warnings=FALSE} -->

<!-- stan_model_mv_cor_nc <- " -->

<!-- data { -->

<!--   int<lower=0> N;               // Number of observations -->

<!--   int<lower=0> J;               // Number of participants -->

<!--   vector[N] X1;                 // Predictor 1 -->

<!--   vector[N] X2;                 // Predictor 2 -->

<!--   vector[N] Y;                  // Response variable -->

<!--   array[N] int participant_id;  // Participant IDs -->

<!-- } -->

<!-- parameters { -->

<!--   vector[3] population_means;                     // Population means for beta0, beta1, beta2 -->

<!--   vector<lower=0>[3] population_sds;              // Population SDs for beta0, beta1, beta2 -->

<!--   cholesky_factor_corr[3] L_population_corr;      // Cholesky factor of correlation matrix for beta0, beta1, and beta2 -->

<!--   matrix[J,3] theta_z;                            // Individual-level beta0, beta1, and beta2 estimates (z-score) -->

<!--   real sigma_mean;                                // Residual population mean (before transformation) -->

<!--   real<lower=0> sigma_sd;                         // Residual population SD (before transformation) -->

<!--   vector[J] sigma_z;                              // Individual-level residuals (z-score, before transformation) -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[J] sigma = log1p_exp(sigma_mean + sigma_sd * sigma_z); -->

<!--   matrix[3,3] L_population_cov = diag_pre_multiply(population_sds,L_population_corr); -->

<!--   matrix[J,3] theta = rep_matrix(population_means',J) + theta_z * L_population_cov'; -->

<!--   vector[J] beta0 = theta[,1]; -->

<!--   vector[J] beta1 = theta[,2]; -->

<!--   vector[J] beta2 = theta[,3]; -->

<!-- } -->

<!-- model { -->

<!--   // Population-level priors -->

<!--   population_means ~ normal(0, 5); -->

<!--   population_sds ~ normal(0, 2.5); -->

<!--   L_population_corr ~ lkj_corr_cholesky(1); -->

<!--   sigma_mean ~ normal(0, 2.5); -->

<!--   sigma_sd ~ normal(0, 2.5); -->

<!--   // Participant-level priors -->

<!--   to_vector(theta_z) ~ std_normal(); -->

<!--   sigma_z ~ std_normal(); -->

<!--   // Likelihood -->

<!--   Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]); -->

<!-- } -->

<!-- generated quantities { -->

<!--   //Compute population correlation matrix from its cholesky factor -->

<!--   corr_matrix[3] population_corr = multiply_lower_tri_self_transpose(L_population_corr); -->

<!-- } -->

<!-- " -->

<!-- # Compile the model -->

<!-- mod_mv_cor_nc <- cmdstan_model(write_stan_file(stan_model_mv_cor_nc)) -->

<!-- # Fit the model -->

<!-- fit_mv_cor_nc <- mod_mv_cor_nc$sample( -->

<!--   data = stan_data, -->

<!--   seed = 123, -->

<!--   chains = 4, -->

<!--   parallel_chains = 4, -->

<!--   refresh = 0, -->

<!--   show_exceptions = FALSE -->

<!-- ) -->

<!-- ``` -->

<!-- As can be seen from the output below, all three of these models produce roughly the same parameter estimates. -->

<!-- ```{r model-5-summary,messages=FALSE,warnings=FALSE} -->

<!-- parameters = c("population_means","population_sds", "population_corr","sigma_mean","sigma_sd","lp__") -->

<!-- fit_mv_cov$summary(variables = parameters) -->

<!-- fit_mv_cor$summary(variables = parameters) -->

<!-- fit_mv_cor_nc$summary(variables = parameters) -->

<!-- ``` -->

<!-- None of these models had any issues with sampling efficiency when applied to these data, but notice that the non-centered version finished much more quickly. In other datasets or for higher-dimensional models, these three parameterisations might differ quite substantially in how efficiently they're able to explore the posterior. So if one parameterisation isn't quite working, try one of the others! -->

</body>
</html>
