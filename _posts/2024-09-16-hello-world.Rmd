---
layout: post
title: "Non-Centered Parameterization in Hierarchical Bayesian Models: Not Just For Univariate Gaussians"
date: 2024-09-18
---

## What is a Hierarchical Bayesian Model?

Hierarchical Bayesian models assess individual variation (e.g., between people in a sample) in a parameter or quantity of interest while simultaneously producing insights about group or population level effects. This type of model is useful because it avoids the need to a) assume the parameter or quantity of interest is identical across the population or b) assume individuals within the same population are completely independent. The hierarchical model recognises that individuals within a population are likely to share certain common characteristics with other members of that population, but also recognises the presence of individual variation within that population.  

Suppose we want to examine the impact of weekly working hours on mental health in a particular population, and that we have multilevel data where multiple observations of both weekly working hours and mental health were taken over time for a set of individuals. A hierarchical Bayesian model would allow us to estimate the impact of working hours on mental health for each individual. However, each individual's estimated effect would be informed not only by that individual's data, but also in part by the estimated effects from other individual's in the population. This phenomenon is sometimes referred to as 'partial pooling' of information. It means that information about the parameter or quantity of interest (in this case, the effect of working hours on mental health) is shared or 'pooled' across individuals in the sample. The 'partial' bit means that an individual's estimate is not completely determined by the information about other individuals' estimates. The model leaves room for individual variation by estimating unique effects for each individual.

The usefulness of hierarchical Bayesian models is well documented elsewhere (REF). The point of this post is not to highlight the virtues of these models. The point is to explain one particularly tricky aspects of actually implementing them.

## What is a Non-Centered Parameterization?

To explain what a non-centered parameterisation is and why it's useful, let's start with a simple example. Let's simulate some multilevel data. In this example, we'll assume we have 500 participants who each produce 5 observations of three variables: X1, X2, and Y. In the code below, the parameters beta0, beta1, beta2, and sigma represent the intercept, linear effect of X1, linear effect of X2, and the residual error respectively. We first assign population-level distributions representing the variation in these parameters across participants. We then randomly sample the parameter values for each participant based on the population-level distributions we defined. Finally, we use the participant-level parameters and the covariates X1 and X2 to randomly generate response variable Y, before creating a list containing that data that can be read into stan.

```{r simulate-data-1}
# Set random seed for reproducibility
set.seed(123)

# Simulate data
N <- 2500  # Total number of observations
J <- 500    # Number of participants
n_j <- N / J  # Number of observations per participant

# Generate participant IDs and predictor variables
participant_id <- rep(1:J, each = n_j)
X1 <- rnorm(N, mean = 0, sd = 1)
X2 <- rnorm(N, mean = 0, sd = 1)

# Population parameter values
beta0_mean <- 2.5   # beta0 population mean
beta1_mean <- 1.5   # Effect of X1 population mean
beta2_mean <- -0.2  # Effect of X2 population mean
beta0_sd <- 1.5     # beta0 population SD
beta1_sd <- 1.3     # Effect of X1  population SD
beta2_sd <- 0.7     # Effect of X2 population SD
sigma_mean <- 0.7   # Residual mean
sigma_sd <- 0.3     # Residual SD

# Simulate variability in parameters across participants
beta0 <- rnorm(n = J, mean = beta0_mean, sd = beta0_sd)
beta1 <- rnorm(n = J, mean = beta1_mean, sd = beta1_sd)
beta2 <- rnorm(n = J, mean = beta2_mean, sd = beta2_sd)
sigma <- rtruncnorm(n = J, a = 0, b = Inf, mean = sigma_mean, sd = sigma_sd)

# Generate response variable
Y <- rep(beta0, each = n_j) + 
  rep(beta1, each = n_j) * X1 + 
  rep(beta2, each = n_j) * X2 + 
  rnorm(N, mean = 0, sd = rep(sigma, each = n_j))

# Prepare data for Stan
stan_data <- list(
  N = N,
  J = J,
  X1 = X1,
  X2 = X2,
  Y = Y,
  participant_id = participant_id
)
```

Now let's define a hierarchical Bayesian model of this data in stan. What makes the model below hierarchical is that the priors on the participant-level parameters defined in the model section depend on the population-level parameters, which themselves are uncertain and therefore estimated by the model. Information from each participant's data flows "up the hierarchy" influencing the population-level parameters via the individual-specific parameters. But information about the population parameters in turn flows back down the hierarchy by constraining the individual parameters to values that are more plausible given the distribution of those parameters in the population.

```{r model-1}
stan_model <- "
data {
  int<lower=0> N;               // Number of observations
  int<lower=0> J;               // Number of participants
  vector[N] X1;                 // Predictor 1
  vector[N] X2;                 // Predictor 2
  vector[N] Y;                  // Response variable
  array[N] int participant_id;  // Participant IDs
}

parameters {
  real beta0_mean;            // Intercept population mean
  real beta1_mean;            // Effect of X1 population mean
  real beta2_mean;            // Effect of X2 population mean
  real<lower=0> beta0_sd;     // Intercept population SD
  real<lower=0> beta1_sd;     // Effect of X1 population SD
  real<lower=0> beta2_sd;     // Effect of X2 population SD
  real<lower=0> sigma_mean;   // Residual mean
  real<lower=0> sigma_sd;     // Residual SD
  vector[J] beta0;            // Participant intercepts
  vector[J] beta1;            // Participant X1 effects
  vector[J] beta2;            // Participant X2 effects
  vector<lower=0>[J] sigma;   // Participant residuals
}

model {
  // Population-level Priors
  beta0_mean ~ normal(0, 5);
  beta1_mean ~ normal(0, 5);
  beta2_mean ~ normal(0, 5);
  beta0_sd ~ normal(0, 2.5);
  beta1_sd ~ normal(0, 2.5);
  beta2_sd ~ normal(0, 2.5);
  sigma_mean ~ normal(0, 2.5);
  sigma_sd ~ normal(0, 2.5);
  
  // Participant-level Priors
  beta0 ~ normal(beta0_mean,beta0_sd);
  beta1 ~ normal(beta1_mean,beta1_sd);
  beta2 ~ normal(beta2_mean,beta2_sd);
  sigma ~ normal(sigma_mean,sigma_sd);
  
  // Likelihood
  Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]);
}
"
```

We can fit the model using the code below.

```{r fit-model-1}
# Compile the model
mod <- cmdstan_model(write_stan_file(stan_model))

# Fit the model
fit <- mod$sample(
  data = stan_data,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 1000
)
```

As you can see from the message after the model finished, there were a lot of divergent transitions. This means the sampler is having difficulty exploring the posterior distribution effectively. This is common when hierarchical models are specified this way, because the scale of each participant-level parameter depends on another parameter in the model. This dependency can create a posterior geometry that is tricky to sample from efficiently. 

This is where the **non-centred parameterisation** can be helpful. It removes this dependency by reparameterising the participant-level parameters. In the model below, a non-centred parameterisation is applied to beta0, beta1, and beta2. (sigma is a little more complicated since it's bounded at 0. We'll get to that next). As you can see, we now estimate z-scores for these three parameters and then in the transformed parameters transform each one back to the appropriate 


```{r model-2}
stan_model_nc <- "
data {
  int<lower=0> N;               // Number of observations
  int<lower=0> J;               // Number of participants
  vector[N] X1;                 // Predictor 1
  vector[N] X2;                 // Predictor 2
  vector[N] Y;                  // Response variable
  array[N] int participant_id;  // Participant IDs
}

parameters {
  real beta0_mean;            // Intercept population mean
  real beta1_mean;            // Effect of X1 population mean
  real beta2_mean;            // Effect of X2 population mean
  real<lower=0> beta0_sd;     // Intercept population SD
  real<lower=0> beta1_sd;     // Effect of X1 population SD
  real<lower=0> beta2_sd;     // Effect of X2 population SD
  real<lower=0> sigma_mean;   // Residual mean
  real<lower=0> sigma_sd;     // Residual SD
  vector[J] beta0_z;          // Participant intercepts (z-score)
  vector[J] beta1_z;          // Participant X1 effects (z-score)
  vector[J] beta2_z;          // Participant X2 effects (z-score)
  vector<lower=0>[J] sigma;   // Participant residuals
}

transformed parameters {
  vector[J] beta0 = beta0_mean + beta0_sd * beta0_z;
  vector[J] beta1 = beta1_mean + beta1_sd * beta1_z;
  vector[J] beta2 = beta2_mean + beta2_sd * beta2_z;
}

model {
  // Population-level Priors
  beta0_mean ~ normal(0, 5);
  beta1_mean ~ normal(0, 5);
  beta2_mean ~ normal(0, 5);
  beta0_sd ~ normal(0, 2.5);
  beta1_sd ~ normal(0, 2.5);
  beta2_sd ~ normal(0, 2.5);
  sigma_mean ~ normal(0, 2.5);
  sigma_sd ~ normal(0, 2.5);
  
  // Participant-level Priors
  beta0 ~ std_normal();
  beta1 ~ std_normal();
  beta2 ~ std_normal();
  sigma ~ normal(sigma_mean,sigma_sd);
  
  // Likelihood
  Y ~ normal(beta0[participant_id] + beta1[participant_id] .* X1 + beta2[participant_id] .* X2, sigma[participant_id]);
}
"

# Compile the model
mod_nc <- cmdstan_model(write_stan_file(stan_model_nc))

# Fit the model
fit_nc <- mod_nc$sample(
  data = stan_data,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 1000
)
```
```





